** cause of errors
1. due to bias - accuracy and underfitting (inadequate model) - need a more
complex model
2. due to variance - precision and overfitting (too complex model) - training
more data or limit the complexity of model

** bias-variance dilemma & number of features -- there is a trade-off, needs
to find a sweet spot. ** 

** the curse of dimensionality -- as the number of features grows, the amount
of data we need to generalize accurately grows exponentially ** think of 10
sample points in each dimension, we need 10^d data points. 

** classification metrics
	accuracy_score (not ideal for skewed data, not ideal for false negative, false positive)

	confusion_matrix in sklearn

				predict class

			negative   | 	positive
                   --------------------------------
actual  | negative	TN	   |	FP	   |
        |------------------------------------------
class	| positive	FN	   |	TP	   |	
        -------------------------------------------

	recall rate: TP / TP + FN (accurate pred / all actual)
	precision rate: TP / TP + FP (accurate pred / all prediction)

	f1_score: 2 * (precision * recall) / (precision + recall)

** regression metrics
	mean absolute error (mae)
	mean squared error (mse)
	r2_score
	explained_variance_score

** learning curve


** Regression -- fun fact: regression is a mis-used word. Its original meaning
comes from the study showing that child height tends to regress to the mean of
population of their parents: tall parents tends to have children that are
shorter than themselves, and shorter parents tends to have children that are
taller than themselves.The coefficient of children's height vs. parent's
height is less than 1. However, later on the fitting of this linear
relationship took the word regression.

** feature selection
	filtering: one way is to use a DT to select features and feed into
learner. DT - information gain based on labels. other ways: entropy,
relevance, independency.
	wrapping: hill climing, randomized opt, forward search, backward
search,
	
